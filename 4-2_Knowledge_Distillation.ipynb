{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先训练老师网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "train = np.array(pd.read_csv('1Ddata_train_ver2.csv'))\n",
    "test = np.array(pd.read_csv('1Ddata_test_ver2.csv'))\n",
    "\n",
    "X_train = train[:,0:-1].reshape((-1, 1, 256))\n",
    "y_train = train[:,-1]\n",
    "X_test = test[:,0:-1].reshape((-1, 1, 256))\n",
    "y_test = test[:,-1]\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test).long())\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataloader = {\"train\": DataLoader(dataset=train_dataset,  # torch TensorDataset format\n",
    "                                  batch_size=batch_size,  # mini batch size\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=int(2),\n",
    "                                  drop_last=True),\n",
    "              \"test\": DataLoader(dataset=test_dataset,  # torch TensorDataset format\n",
    "                                 batch_size=batch_size,  # mini batch size\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=int(2),\n",
    "                                 drop_last=False)}\n",
    "classes = ('surface1', 'surface2', 'surface3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherNet, self).__init__()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        # Instantiate dense layers\n",
    "        self.fc1 = nn.Linear(16*128, 120)  #10x7x7 num_channel x height x width , orignal H/W 28 after two pooling, H/W = 7\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)        \n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)       \n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "def train_teacher(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')\n",
    "\n",
    "\n",
    "def test_teacher(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_main(dataloader):\n",
    "    epochs = 60\n",
    "#     batch_size = 64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#     #load data\n",
    "#     train = np.array(pd.read_csv('1Ddata_train_ver2.csv'))\n",
    "#     test = np.array(pd.read_csv('1Ddata_test_ver2.csv'))\n",
    "\n",
    "#     X_train = train[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_train = train[:,-1]\n",
    "#     X_test = test[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_test = test[:,-1]\n",
    "\n",
    "#     train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#     test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test).long())\n",
    "\n",
    "#     batch_size = 50\n",
    "\n",
    "#     dataloader = {\"train\": DataLoader(dataset=train_dataset,  # torch TensorDataset format\n",
    "#                                       batch_size=batch_size,  # mini batch size\n",
    "#                                       shuffle=True,\n",
    "#                                       num_workers=int(2),\n",
    "#                                       drop_last=True),\n",
    "#                   \"test\": DataLoader(dataset=test_dataset,  # torch TensorDataset format\n",
    "#                                      batch_size=batch_size,  # mini batch size\n",
    "#                                      shuffle=True,\n",
    "#                                      num_workers=int(2),\n",
    "#                                      drop_last=False)}\n",
    "#     classes = ('surface1', 'surface2', 'surface3')    \n",
    "\n",
    "    #     train_loader = torch.utils.data.DataLoader(\n",
    "    #         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "    #                        transform=transforms.Compose([\n",
    "    #                            transforms.ToTensor(),\n",
    "    #                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #                        ])),\n",
    "    #         batch_size=batch_size, shuffle=True)\n",
    "    #     test_loader = torch.utils.data.DataLoader(\n",
    "    #         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "    #             transforms.ToTensor(),\n",
    "    #             transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #         ])),\n",
    "    #         batch_size=1000, shuffle=True)\n",
    "\n",
    "    model = TeacherNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    teacher_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_teacher(model, device, dataloader[\"train\"], optimizer, epoch)\n",
    "        loss, acc = test_teacher(model, device, dataloader[\"test\"])\n",
    "\n",
    "        teacher_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"teacher.pt\")\n",
    "    return model, teacher_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0468, accuracy: 217/449 (48%)\n",
      "Train epoch 2: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8851, accuracy: 277/449 (62%)\n",
      "Train epoch 3: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6409, accuracy: 329/449 (73%)\n",
      "Train epoch 4: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4794, accuracy: 360/449 (80%)\n",
      "Train epoch 5: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5542, accuracy: 318/449 (71%)\n",
      "Train epoch 6: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4179, accuracy: 370/449 (82%)\n",
      "Train epoch 7: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5307, accuracy: 328/449 (73%)\n",
      "Train epoch 8: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5816, accuracy: 321/449 (71%)\n",
      "Train epoch 9: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4063, accuracy: 367/449 (82%)\n",
      "Train epoch 10: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3906, accuracy: 380/449 (85%)\n",
      "Train epoch 11: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3894, accuracy: 369/449 (82%)\n",
      "Train epoch 12: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5182, accuracy: 336/449 (75%)\n",
      "Train epoch 13: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3922, accuracy: 376/449 (84%)\n",
      "Train epoch 14: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4282, accuracy: 358/449 (80%)\n",
      "Train epoch 15: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3960, accuracy: 366/449 (82%)\n",
      "Train epoch 16: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3774, accuracy: 373/449 (83%)\n",
      "Train epoch 17: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5569, accuracy: 334/449 (74%)\n",
      "Train epoch 18: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5871, accuracy: 328/449 (73%)\n",
      "Train epoch 19: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3934, accuracy: 366/449 (82%)\n",
      "Train epoch 20: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4059, accuracy: 369/449 (82%)\n",
      "Train epoch 21: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3922, accuracy: 369/449 (82%)\n",
      "Train epoch 22: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3764, accuracy: 374/449 (83%)\n",
      "Train epoch 23: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4604, accuracy: 352/449 (78%)\n",
      "Train epoch 24: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4089, accuracy: 364/449 (81%)\n",
      "Train epoch 25: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4285, accuracy: 357/449 (80%)\n",
      "Train epoch 26: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3908, accuracy: 371/449 (83%)\n",
      "Train epoch 27: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3783, accuracy: 376/449 (84%)\n",
      "Train epoch 28: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3759, accuracy: 372/449 (83%)\n",
      "Train epoch 29: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3911, accuracy: 370/449 (82%)\n",
      "Train epoch 30: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3802, accuracy: 371/449 (83%)\n",
      "Train epoch 31: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4021, accuracy: 367/449 (82%)\n",
      "Train epoch 32: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4366, accuracy: 357/449 (80%)\n",
      "Train epoch 33: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4429, accuracy: 359/449 (80%)\n",
      "Train epoch 34: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3820, accuracy: 371/449 (83%)\n",
      "Train epoch 35: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3818, accuracy: 369/449 (82%)\n",
      "Train epoch 36: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3898, accuracy: 369/449 (82%)\n",
      "Train epoch 37: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4519, accuracy: 358/449 (80%)\n",
      "Train epoch 38: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3971, accuracy: 364/449 (81%)\n",
      "Train epoch 39: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3776, accuracy: 375/449 (84%)\n",
      "Train epoch 40: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4578, accuracy: 355/449 (79%)\n",
      "Train epoch 41: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3878, accuracy: 365/449 (81%)\n",
      "Train epoch 42: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4007, accuracy: 371/449 (83%)\n",
      "Train epoch 43: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3874, accuracy: 370/449 (82%)\n",
      "Train epoch 44: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5643, accuracy: 329/449 (73%)\n",
      "Train epoch 45: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4088, accuracy: 369/449 (82%)\n",
      "Train epoch 46: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3867, accuracy: 367/449 (82%)\n",
      "Train epoch 47: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3884, accuracy: 371/449 (83%)\n",
      "Train epoch 48: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3808, accuracy: 371/449 (83%)\n",
      "Train epoch 49: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5501, accuracy: 339/449 (76%)\n",
      "Train epoch 50: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5407, accuracy: 333/449 (74%)\n",
      "Train epoch 51: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3882, accuracy: 368/449 (82%)\n",
      "Train epoch 52: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3811, accuracy: 373/449 (83%)\n",
      "Train epoch 53: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3827, accuracy: 373/449 (83%)\n",
      "Train epoch 54: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3790, accuracy: 375/449 (84%)\n",
      "Train epoch 55: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4290, accuracy: 367/449 (82%)\n",
      "Train epoch 56: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3853, accuracy: 372/449 (83%)\n",
      "Train epoch 57: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.3865, accuracy: 372/449 (83%)\n",
      "Train epoch 58: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4460, accuracy: 358/449 (80%)\n",
      "Train epoch 59: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.4310, accuracy: 362/449 (81%)\n",
      "Train epoch 60: 1000/1049, [------------------------------------------------>  ] 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: average loss: 0.4033, accuracy: 366/449 (82%)\n"
     ]
    }
   ],
   "source": [
    "# 训练教师网络\n",
    "\n",
    "teacher_model, teacher_history = teacher_main(dataloader)\n",
    "# teacher_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小插曲，看看老师的暗知识 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def softmax_t(x, t):\n",
    "    x_exp = np.exp(x / t)\n",
    "    return x_exp / np.sum(x_exp)\n",
    "\n",
    "# test_loader_bs1 = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])),\n",
    "#     batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (NO softmax): [ 0.7889364  0.1060057 -1.9244622]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(dataloader[\"test\"]))\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    output = teacher_model(data)\n",
    "\n",
    "test_x = data.cpu().numpy()\n",
    "y_out = output.cpu().numpy()\n",
    "y_out = y_out[0, ::]\n",
    "print('Output (NO softmax):', y_out)\n",
    "\n",
    "\n",
    "\n",
    "# plt.subplot(3, 1, 1)\n",
    "# plt.imshow(test_x[0, 0, ::])\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(list(range(3)), softmax_t(y_out, 1), width=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(list(range(3)), softmax_t(y_out, 10), width=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 让老师教学生网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256 * 1, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键，定义kd的loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation(y, labels, teacher_scores, temp, alpha):\n",
    "    return nn.KLDivLoss()(F.log_softmax(y / temp, dim=1), F.softmax(teacher_scores / temp, dim=1)) * (\n",
    "            temp * temp * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_kd(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "#         data_stu = data.view(-1,256*1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         print(output.shape)\n",
    "#         print(target.shape)\n",
    "        teacher_output = teacher_model(data)\n",
    "        teacher_output = teacher_output.detach()  # 切断老师网络的反向传播，感谢B站“淡淡的落”的提醒\n",
    "#         print(teacher_output.shape)\n",
    "        loss = distillation(output, target, teacher_output, temp=5.0, alpha=0.7)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')\n",
    "\n",
    "\n",
    "def test_student_kd(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "#             data_stu = data.view(-1,256*1)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_kd_main(dataloader):\n",
    "    epochs = 100\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    \n",
    "#     #load data\n",
    "#     train = np.array(pd.read_csv('1Ddata_train_ver2.csv'))\n",
    "#     test = np.array(pd.read_csv('1Ddata_test_ver2.csv'))\n",
    "\n",
    "#     #train = train.reshape((train.shape[0], 1, 257))\n",
    "#     #test = test.reshape((test.shape[0], 1, 257))\n",
    "\n",
    "#     X_train = train[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_train = train[:,-1]\n",
    "#     X_test = test[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_test = test[:,-1]\n",
    "\n",
    "#     #reproducibility\n",
    "#     torch.manual_seed(0)\n",
    "\n",
    "\n",
    "#     train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#     test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test).long())\n",
    "\n",
    "#     batch_size = 50\n",
    "\n",
    "#     dataloader = {\"train\": DataLoader(dataset=train_dataset,  # torch TensorDataset format\n",
    "#                                       batch_size=batch_size,  # mini batch size\n",
    "#                                       shuffle=True,\n",
    "#                                       num_workers=int(2),\n",
    "#                                       drop_last=False),\n",
    "#                   \"test\": DataLoader(dataset=test_dataset,  # torch TensorDataset format\n",
    "#                                      batch_size=batch_size,  # mini batch size\n",
    "#                                      shuffle=True,\n",
    "#                                      num_workers=int(2),\n",
    "#                                      drop_last=False)}\n",
    "#     classes = ('surface1', 'surface2', 'surface3') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                        ])),\n",
    "#         batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('../data/MNIST', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])),\n",
    "#         batch_size=1000, shuffle=True)\n",
    "\n",
    "    model = StudentNet().to(device)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters())\n",
    "    \n",
    "    student_history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_student_kd(model, device, dataloader[\"train\"], optimizer, epoch)\n",
    "        loss, acc = test_student_kd(model, device, dataloader[\"test\"])\n",
    "        student_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"student_kd.pt\")\n",
    "    return model, student_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhshi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2748: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0867, accuracy: 149/449 (33%)\n",
      "Train epoch 2: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0697, accuracy: 159/449 (35%)\n",
      "Train epoch 3: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0829, accuracy: 153/449 (34%)\n",
      "Train epoch 4: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9892, accuracy: 222/449 (49%)\n",
      "Train epoch 5: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.2927, accuracy: 168/449 (37%)\n",
      "Train epoch 6: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.1747, accuracy: 172/449 (38%)\n",
      "Train epoch 7: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.3899, accuracy: 161/449 (36%)\n",
      "Train epoch 8: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9091, accuracy: 235/449 (52%)\n",
      "Train epoch 9: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.1092, accuracy: 206/449 (46%)\n",
      "Train epoch 10: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.8600, accuracy: 167/449 (37%)\n",
      "Train epoch 11: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9126, accuracy: 258/449 (57%)\n",
      "Train epoch 12: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.1412, accuracy: 236/449 (53%)\n",
      "Train epoch 13: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9122, accuracy: 259/449 (58%)\n",
      "Train epoch 14: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0094, accuracy: 244/449 (54%)\n",
      "Train epoch 15: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.4047, accuracy: 236/449 (53%)\n",
      "Train epoch 16: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9980, accuracy: 230/449 (51%)\n",
      "Train epoch 17: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.2702, accuracy: 250/449 (56%)\n",
      "Train epoch 18: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8481, accuracy: 272/449 (61%)\n",
      "Train epoch 19: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.8614, accuracy: 212/449 (47%)\n",
      "Train epoch 20: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9613, accuracy: 282/449 (63%)\n",
      "Train epoch 21: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9107, accuracy: 279/449 (62%)\n",
      "Train epoch 22: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0429, accuracy: 285/449 (63%)\n",
      "Train epoch 23: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8772, accuracy: 278/449 (62%)\n",
      "Train epoch 24: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.2615, accuracy: 272/449 (61%)\n",
      "Train epoch 25: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8252, accuracy: 292/449 (65%)\n",
      "Train epoch 26: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.2586, accuracy: 271/449 (60%)\n",
      "Train epoch 27: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9625, accuracy: 262/449 (58%)\n",
      "Train epoch 28: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8285, accuracy: 282/449 (63%)\n",
      "Train epoch 29: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7870, accuracy: 305/449 (68%)\n",
      "Train epoch 30: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.3914, accuracy: 268/449 (60%)\n",
      "Train epoch 31: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.2523, accuracy: 264/449 (59%)\n",
      "Train epoch 32: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7389, accuracy: 297/449 (66%)\n",
      "Train epoch 33: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7467, accuracy: 298/449 (66%)\n",
      "Train epoch 34: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0837, accuracy: 285/449 (63%)\n",
      "Train epoch 35: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8144, accuracy: 286/449 (64%)\n",
      "Train epoch 36: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7720, accuracy: 315/449 (70%)\n",
      "Train epoch 37: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7414, accuracy: 317/449 (71%)\n",
      "Train epoch 38: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7192, accuracy: 316/449 (70%)\n",
      "Train epoch 39: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7813, accuracy: 291/449 (65%)\n",
      "Train epoch 40: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6836, accuracy: 303/449 (67%)\n",
      "Train epoch 41: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7105, accuracy: 319/449 (71%)\n",
      "Train epoch 42: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6971, accuracy: 314/449 (70%)\n",
      "Train epoch 43: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7034, accuracy: 300/449 (67%)\n",
      "Train epoch 44: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7087, accuracy: 308/449 (69%)\n",
      "Train epoch 45: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7387, accuracy: 313/449 (70%)\n",
      "Train epoch 46: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7694, accuracy: 314/449 (70%)\n",
      "Train epoch 47: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0178, accuracy: 292/449 (65%)\n",
      "Train epoch 48: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6950, accuracy: 312/449 (69%)\n",
      "Train epoch 49: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6701, accuracy: 312/449 (69%)\n",
      "Train epoch 50: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8880, accuracy: 311/449 (69%)\n",
      "Train epoch 51: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7154, accuracy: 298/449 (66%)\n",
      "Train epoch 52: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9265, accuracy: 309/449 (69%)\n",
      "Train epoch 53: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9644, accuracy: 294/449 (65%)\n",
      "Train epoch 54: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9523, accuracy: 290/449 (65%)\n",
      "Train epoch 55: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8665, accuracy: 309/449 (69%)\n",
      "Train epoch 56: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7245, accuracy: 296/449 (66%)\n",
      "Train epoch 57: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6969, accuracy: 312/449 (69%)\n",
      "Train epoch 58: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6525, accuracy: 316/449 (70%)\n",
      "Train epoch 59: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7370, accuracy: 294/449 (65%)\n",
      "Train epoch 60: 1000/1049, [------------------------------------------------>  ] 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: average loss: 0.7170, accuracy: 298/449 (66%)\n",
      "Train epoch 61: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6705, accuracy: 316/449 (70%)\n",
      "Train epoch 62: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8896, accuracy: 301/449 (67%)\n",
      "Train epoch 63: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7108, accuracy: 319/449 (71%)\n",
      "Train epoch 64: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7348, accuracy: 300/449 (67%)\n",
      "Train epoch 65: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6900, accuracy: 322/449 (72%)\n",
      "Train epoch 66: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0199, accuracy: 288/449 (64%)\n",
      "Train epoch 67: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6689, accuracy: 314/449 (70%)\n",
      "Train epoch 68: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7614, accuracy: 292/449 (65%)\n",
      "Train epoch 69: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6929, accuracy: 314/449 (70%)\n",
      "Train epoch 70: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6695, accuracy: 315/449 (70%)\n",
      "Train epoch 71: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6674, accuracy: 325/449 (72%)\n",
      "Train epoch 72: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6823, accuracy: 322/449 (72%)\n",
      "Train epoch 73: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6787, accuracy: 328/449 (73%)\n",
      "Train epoch 74: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7416, accuracy: 296/449 (66%)\n",
      "Train epoch 75: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8395, accuracy: 307/449 (68%)\n",
      "Train epoch 76: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6759, accuracy: 324/449 (72%)\n",
      "Train epoch 77: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7359, accuracy: 324/449 (72%)\n",
      "Train epoch 78: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7614, accuracy: 319/449 (71%)\n",
      "Train epoch 79: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9420, accuracy: 302/449 (67%)\n",
      "Train epoch 80: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8628, accuracy: 313/449 (70%)\n",
      "Train epoch 81: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7529, accuracy: 327/449 (73%)\n",
      "Train epoch 82: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6401, accuracy: 327/449 (73%)\n",
      "Train epoch 83: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7499, accuracy: 324/449 (72%)\n",
      "Train epoch 84: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6508, accuracy: 324/449 (72%)\n",
      "Train epoch 85: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7024, accuracy: 303/449 (67%)\n",
      "Train epoch 86: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8667, accuracy: 310/449 (69%)\n",
      "Train epoch 87: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6889, accuracy: 325/449 (72%)\n",
      "Train epoch 88: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7097, accuracy: 310/449 (69%)\n",
      "Train epoch 89: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8287, accuracy: 311/449 (69%)\n",
      "Train epoch 90: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6696, accuracy: 309/449 (69%)\n",
      "Train epoch 91: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6648, accuracy: 317/449 (71%)\n",
      "Train epoch 92: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7343, accuracy: 299/449 (67%)\n",
      "Train epoch 93: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6760, accuracy: 307/449 (68%)\n",
      "Train epoch 94: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6693, accuracy: 318/449 (71%)\n",
      "Train epoch 95: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7366, accuracy: 301/449 (67%)\n",
      "Train epoch 96: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8960, accuracy: 297/449 (66%)\n",
      "Train epoch 97: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7922, accuracy: 315/449 (70%)\n",
      "Train epoch 98: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6777, accuracy: 324/449 (72%)\n",
      "Train epoch 99: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6467, accuracy: 319/449 (71%)\n",
      "Train epoch 100: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7038, accuracy: 325/449 (72%)\n"
     ]
    }
   ],
   "source": [
    "student_kd_model, student_kd_history = student_kd_main(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 让学生自己学，不使用KD\n",
    "def train_student(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    trained_samples = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "#         data = data.view(-1,256*1) #change data shape (MLP needed)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(output)\n",
    "        # a\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples += len(data)\n",
    "        progress = math.ceil(batch_idx / len(train_loader) * 50)\n",
    "        print(\"\\rTrain epoch %d: %d/%d, [%-51s] %d%%\" %\n",
    "              (epoch, trained_samples, len(train_loader.dataset),\n",
    "               '-' * progress + '>', progress * 2), end='')\n",
    "\n",
    "\n",
    "def test_student(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "#             data = data.view(-1,256*1) #change data shape (MLP needed)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest: average loss: {:.4f}, accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_main(dataloader):\n",
    "    epochs = 100\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     train = np.array(pd.read_csv('1Ddata_train_ver2.csv'))\n",
    "#     test = np.array(pd.read_csv('1Ddata_test_ver2.csv'))\n",
    "\n",
    "#     #train = train.reshape((train.shape[0], 1, 257))\n",
    "#     #test = test.reshape((test.shape[0], 1, 257))\n",
    "\n",
    "#     X_train = train[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_train = train[:,-1]\n",
    "#     X_test = test[:,0:-1].reshape((-1, 1, 256))\n",
    "#     y_test = test[:,-1]\n",
    "\n",
    "#     #reproducibility\n",
    "#     torch.manual_seed(0)\n",
    "\n",
    "\n",
    "#     train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#     test_dataset = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test).long())\n",
    "\n",
    "#     batch_size = 50\n",
    "\n",
    "#     dataloader = {\"train\": DataLoader(dataset=train_dataset,  # torch TensorDataset format\n",
    "#                                       batch_size=batch_size,  # mini batch size\n",
    "#                                       shuffle=True,\n",
    "#                                       num_workers=int(2),\n",
    "#                                       drop_last=False),\n",
    "#                   \"test\": DataLoader(dataset=test_dataset,  # torch TensorDataset format\n",
    "#                                      batch_size=batch_size,  # mini batch size\n",
    "#                                      shuffle=True,\n",
    "#                                      num_workers=int(2),\n",
    "#                                      drop_last=False)}\n",
    "#     classes = ('surface1', 'surface2', 'surface3')\n",
    "\n",
    "    model = StudentNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    student_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_student(model, device, dataloader['train'], optimizer, epoch)\n",
    "        loss, acc = test_student(model, device, dataloader['test'])\n",
    "        student_history.append((loss, acc))\n",
    "\n",
    "    torch.save(model.state_dict(), \"student.pt\")\n",
    "    return model, student_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0936, accuracy: 149/449 (33%)\n",
      "Train epoch 2: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0630, accuracy: 234/449 (52%)\n",
      "Train epoch 3: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 1.0030, accuracy: 238/449 (53%)\n",
      "Train epoch 4: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.9363, accuracy: 247/449 (55%)\n",
      "Train epoch 5: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8828, accuracy: 245/449 (55%)\n",
      "Train epoch 6: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8226, accuracy: 276/449 (61%)\n",
      "Train epoch 7: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.8857, accuracy: 249/449 (55%)\n",
      "Train epoch 8: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7773, accuracy: 263/449 (59%)\n",
      "Train epoch 9: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7184, accuracy: 298/449 (66%)\n",
      "Train epoch 10: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7269, accuracy: 295/449 (66%)\n",
      "Train epoch 11: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7198, accuracy: 283/449 (63%)\n",
      "Train epoch 12: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7271, accuracy: 297/449 (66%)\n",
      "Train epoch 13: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6491, accuracy: 316/449 (70%)\n",
      "Train epoch 14: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6420, accuracy: 324/449 (72%)\n",
      "Train epoch 15: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6579, accuracy: 311/449 (69%)\n",
      "Train epoch 16: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6229, accuracy: 331/449 (74%)\n",
      "Train epoch 17: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6138, accuracy: 321/449 (71%)\n",
      "Train epoch 18: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6156, accuracy: 329/449 (73%)\n",
      "Train epoch 19: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6428, accuracy: 299/449 (67%)\n",
      "Train epoch 20: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6140, accuracy: 331/449 (74%)\n",
      "Train epoch 21: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6006, accuracy: 322/449 (72%)\n",
      "Train epoch 22: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.7059, accuracy: 302/449 (67%)\n",
      "Train epoch 23: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6750, accuracy: 299/449 (67%)\n",
      "Train epoch 24: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6266, accuracy: 308/449 (69%)\n",
      "Train epoch 25: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6579, accuracy: 312/449 (69%)\n",
      "Train epoch 26: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6239, accuracy: 326/449 (73%)\n",
      "Train epoch 27: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6140, accuracy: 315/449 (70%)\n",
      "Train epoch 28: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5939, accuracy: 332/449 (74%)\n",
      "Train epoch 29: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6013, accuracy: 333/449 (74%)\n",
      "Train epoch 30: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6424, accuracy: 310/449 (69%)\n",
      "Train epoch 31: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6042, accuracy: 315/449 (70%)\n",
      "Train epoch 32: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6466, accuracy: 305/449 (68%)\n",
      "Train epoch 33: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5977, accuracy: 332/449 (74%)\n",
      "Train epoch 34: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5891, accuracy: 327/449 (73%)\n",
      "Train epoch 35: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5999, accuracy: 314/449 (70%)\n",
      "Train epoch 36: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6693, accuracy: 313/449 (70%)\n",
      "Train epoch 37: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5846, accuracy: 325/449 (72%)\n",
      "Train epoch 38: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5888, accuracy: 321/449 (71%)\n",
      "Train epoch 39: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6073, accuracy: 316/449 (70%)\n",
      "Train epoch 40: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5895, accuracy: 325/449 (72%)\n",
      "Train epoch 41: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6252, accuracy: 323/449 (72%)\n",
      "Train epoch 42: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6034, accuracy: 334/449 (74%)\n",
      "Train epoch 43: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5873, accuracy: 323/449 (72%)\n",
      "Train epoch 44: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5999, accuracy: 319/449 (71%)\n",
      "Train epoch 45: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5952, accuracy: 321/449 (71%)\n",
      "Train epoch 46: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6934, accuracy: 306/449 (68%)\n",
      "Train epoch 47: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5817, accuracy: 328/449 (73%)\n",
      "Train epoch 48: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5971, accuracy: 334/449 (74%)\n",
      "Train epoch 49: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6562, accuracy: 312/449 (69%)\n",
      "Train epoch 50: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5793, accuracy: 332/449 (74%)\n",
      "Train epoch 51: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5899, accuracy: 335/449 (75%)\n",
      "Train epoch 52: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6378, accuracy: 317/449 (71%)\n",
      "Train epoch 53: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6255, accuracy: 311/449 (69%)\n",
      "Train epoch 54: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5999, accuracy: 316/449 (70%)\n",
      "Train epoch 55: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5870, accuracy: 335/449 (75%)\n",
      "Train epoch 56: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5936, accuracy: 335/449 (75%)\n",
      "Train epoch 57: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5902, accuracy: 324/449 (72%)\n",
      "Train epoch 58: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5851, accuracy: 337/449 (75%)\n",
      "Train epoch 59: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6152, accuracy: 320/449 (71%)\n",
      "Train epoch 60: 1000/1049, [------------------------------------------------>  ] 96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: average loss: 0.6359, accuracy: 316/449 (70%)\n",
      "Train epoch 61: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5776, accuracy: 337/449 (75%)\n",
      "Train epoch 62: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6314, accuracy: 319/449 (71%)\n",
      "Train epoch 63: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5878, accuracy: 318/449 (71%)\n",
      "Train epoch 64: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5840, accuracy: 337/449 (75%)\n",
      "Train epoch 65: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6248, accuracy: 312/449 (69%)\n",
      "Train epoch 66: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5976, accuracy: 328/449 (73%)\n",
      "Train epoch 67: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5757, accuracy: 335/449 (75%)\n",
      "Train epoch 68: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5760, accuracy: 331/449 (74%)\n",
      "Train epoch 69: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5758, accuracy: 335/449 (75%)\n",
      "Train epoch 70: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5725, accuracy: 332/449 (74%)\n",
      "Train epoch 71: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5993, accuracy: 326/449 (73%)\n",
      "Train epoch 72: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5983, accuracy: 327/449 (73%)\n",
      "Train epoch 73: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6095, accuracy: 318/449 (71%)\n",
      "Train epoch 74: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5792, accuracy: 322/449 (72%)\n",
      "Train epoch 75: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5713, accuracy: 332/449 (74%)\n",
      "Train epoch 76: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5767, accuracy: 325/449 (72%)\n",
      "Train epoch 77: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6249, accuracy: 325/449 (72%)\n",
      "Train epoch 78: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5804, accuracy: 336/449 (75%)\n",
      "Train epoch 79: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5706, accuracy: 328/449 (73%)\n",
      "Train epoch 80: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5847, accuracy: 331/449 (74%)\n",
      "Train epoch 81: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5806, accuracy: 319/449 (71%)\n",
      "Train epoch 82: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5731, accuracy: 338/449 (75%)\n",
      "Train epoch 83: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5686, accuracy: 333/449 (74%)\n",
      "Train epoch 84: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5687, accuracy: 335/449 (75%)\n",
      "Train epoch 85: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5844, accuracy: 332/449 (74%)\n",
      "Train epoch 86: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.6531, accuracy: 318/449 (71%)\n",
      "Train epoch 87: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5690, accuracy: 328/449 (73%)\n",
      "Train epoch 88: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5890, accuracy: 320/449 (71%)\n",
      "Train epoch 89: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5844, accuracy: 320/449 (71%)\n",
      "Train epoch 90: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5706, accuracy: 336/449 (75%)\n",
      "Train epoch 91: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5811, accuracy: 320/449 (71%)\n",
      "Train epoch 92: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5849, accuracy: 336/449 (75%)\n",
      "Train epoch 93: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5922, accuracy: 330/449 (73%)\n",
      "Train epoch 94: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5698, accuracy: 338/449 (75%)\n",
      "Train epoch 95: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5686, accuracy: 333/449 (74%)\n",
      "Train epoch 96: 1000/1049, [------------------------------------------------>  ] 96%\n",
      "Test: average loss: 0.5652, accuracy: 339/449 (76%)\n",
      "Train epoch 97: 1000/1049, [------------------------------------------------>  ] 96%"
     ]
    }
   ],
   "source": [
    "student_simple_model, student_simple_history = student_main(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "\n",
    "# plt.plot(x, [teacher_history[i][1] for i in range(epochs)], label='teacher')\n",
    "# plt.plot(x, [student_kd_history[i][1] for i in range(epochs)], label='student with KD')\n",
    "# plt.plot(x, [student_simple_history[i][1] for i in range(epochs)], label='student without KD')\n",
    "\n",
    "# plt.title('Test accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(x, [teacher_history[i][0] for i in range(epochs)], label='teacher')\n",
    "# plt.plot(x, [student_kd_history[i][0] for i in range(epochs)], label='student with KD')\n",
    "# plt.plot(x, [student_simple_history[i][0] for i in range(epochs)], label='student without KD')\n",
    "\n",
    "# plt.title('Test loss')\n",
    "# plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "x = list(range(1, epochs+1))\n",
    "\n",
    "#multiple plots in 1 fig\n",
    "fig,ax=plt.subplots(2,1,figsize=(12,12),dpi=80)\n",
    "\n",
    "\n",
    "ax[0].plot([teacher_history[i][1] for i in range(epochs)], label='teacher')\n",
    "ax[0].plot([student_kd_history[i][1] for i in range(epochs)], label='student with KD')\n",
    "ax[0].plot([student_simple_history[i][1] for i in range(epochs)], label='student without KD')\n",
    "\n",
    "ax[0].set_title('Test accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "ax[1].plot([teacher_history[i][0] for i in range(epochs)], label='teacher')\n",
    "ax[1].plot([student_kd_history[i][0] for i in range(epochs)], label='student with KD')\n",
    "ax[1].plot([student_simple_history[i][0] for i in range(epochs)], label='student without KD')\n",
    "\n",
    "ax[1].set_title('Test loss')\n",
    "ax[1].legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
